A recent study concludes that medical research papers often do not describe placebos adequately. The authors believe that this could cause underreporting of harms and overreporting of benefits. To find out whether a drug or medical intervention works, researchers must pit it against a placebo. If the experimental condition cannot outperform the placebo, then they cannot consider it to be effective. However, a recent study explains how placebos are not as benign as many people believe. The researchers behind the new study ask whether scientists take the time to explain the exact formulations of their placebo treatments when publishing their results. Placebos come in many forms, including saline injections, sham surgery, and tablets or capsules of any shape, size, or color. They also contain a range of ingredients, sometimes including a chemical to mimic the taste or feel of the active drug. As the authors of the new study explain, “ [a]ll of these differences can influence how effective they are. ” The authors outline an example where a particular placebo skewed the results of several studies. In studies that investigated oseltamivir, which people may know by its brand name Tamiflu, scientists often added dehydrocholic acid to the placebo. Dehydrocholic acid has a bitter taste, as does oseltamivir. The researchers chose to add this chemical to the placebo so that the participants would not know whether they had received the active drug or the placebo. However, both dehydrocholic acid and oseltamivir cause gastrointestinal side effects. When scientists attempted to calculate the rate of gastrointestinal side effects due to oseltamivir, they compared them with side effects from the placebo. As the placebo also caused these types of symptoms, scientists underestimated the overall gastrointestinal side effect rate for oseltamivir. Another issue is mismatching between a placebo and the experimental condition — in other words, they do not look, taste, or feel the same. In these cases, a participant can easily determine that they are not receiving the experimental drug. If the participant believes that they are “ just receiving a placebo, ” they might not expect any benefits. This awareness has the potential to alter results, making the experimental drug appear more beneficial than it truly is. A final example of the accidental influence of placebos concerns olive oil. In early studies investigating cholesterol-lowering drugs, scientists often used olive oil as a placebo. Later, it became clear that olive oil itself lowers cholesterol. Taking these points together, it is increasingly clear that placebos can have a significant influence on a study’s results. With this in mind, a group of researchers from the University of Oxford in the United Kingdom decided to investigate how often authors accurately report placebo interventions in scientific papers. They recently published their findings in the European Journal of Clinical Investigation. To investigate, the scientists trawled through papers that the top six general medical journals, including JAMA and BMJ, had published in 2018. They collected all of the papers that used randomized placebo or sham procedures. This search produced 94 papers, which they supplemented with a further 100 papers from any other journals using the same criteria. They assessed the authors’ description of the placebo using current best practice guidelines, which are called the Template for Intervention Description and Replication checklist (TIDieR). The TIDieR includes 12 checklist items for reporting placebo or sham procedures. The team found that in all of the papers from the top journals, the authors named the placebo. In most cases, they also explained how they performed the sham or placebo and how much they administered to the participants. However, on average, the papers covered just eight of the 12 items in the checklist. Only 8. 5% of the top journal articles explained why the scientists had chosen the specific placebo, and less than half reported who provided the sham procedure. In the second batch of 100 articles, the reporting was poorer. On average, the study authors reported just six of the 12 items in the TIDieR checklist. “ It is impossible to say how often placebo components influence what the apparent benefit of the new treatment is until such components are reported adequately. As this study shows, they rarely are. ” Co-lead author Dr Rebecca WebsterPlacebo and sham interventions can have a real and measurable effect on a participant, so it is vital that researchers fully explain how they perform them. As it stands, assessing how much a drug benefits or harms a person remains partially hidden behind missing information about placebos. The authors of the study hope that in the future, researchers will “ investigate why current guidelines for reporting ‘active’ interventions (TIDieR) are rarely used, even among journals such as the BMJ who allegedly require its use. ”